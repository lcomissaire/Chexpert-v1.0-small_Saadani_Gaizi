{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADJUSTING AND TESTING OF THE DENSNET121 CONVOLUTIONAL NEURAL NETWORK WITH CheXpert DATA\n",
    "\n",
    "- Importing packages\n",
    "- Importing Data\n",
    "- Adjusting Densnet121 CNN\n",
    "- Creation of the training algorithm\n",
    "- Testing (ploting of the loss and ROC curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch.utils.data.dataset import random_split\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PATIENTS = 64540\n",
    "CLASSES = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n",
    "           'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "           'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
    "csv_train_path = '../CheXpert-v1.0-small/train.csv'\n",
    "csv_valid_path = '../CheXpert-v1.0-small/valid.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Densenet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def densenet121():\n",
    "    DN121 = models.densenet121()\n",
    "    n_feats = DN121.classifier.in_features\n",
    "    DN121.classifier = nn.Sequential(\n",
    "        nn.Linear(n_feats, 14),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    return DN121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator(Dataset):\n",
    "    \n",
    "    def __init__ (self, path_table):\n",
    "        #U-Ones approach\n",
    "        self.imgs_path = []\n",
    "        self.labels = []\n",
    "        \n",
    "        with open(path_table, \"r\") as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            next(csv_reader)\n",
    "            for line in csv_reader:\n",
    "                label = line[5:]\n",
    "                for i in range(14):\n",
    "                    if label[i]:\n",
    "                        c = float(label[i])\n",
    "                        if c == 1:\n",
    "                            label[i] = 1\n",
    "                        elif c == -1:\n",
    "                            label[i] = 1\n",
    "                        else:\n",
    "                            label[i] = 0\n",
    "                    else:\n",
    "                        label[i] = 0\n",
    "\n",
    "                self.imgs_path.append('../' + line[0])\n",
    "                self.labels.append(label)\n",
    "                \n",
    "        self.preprocess = transforms.Compose([\n",
    "                            transforms.Resize([256,256]),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                            ])\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img_path = self.imgs_path[index]\n",
    "        \n",
    "        image_data = Image.open(img_path).convert('RGB')\n",
    "        image_label= torch.FloatTensor(self.labels[index])\n",
    "        \n",
    "        image_data = self.preprocess(image_data)\n",
    "        \n",
    "        return image_data, image_label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINIG ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs):\n",
    "\n",
    "    loss = torch.nn.BCELoss(size_average = True)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=5, mode='min')\n",
    "    \n",
    "    loss_mean_min = 1e100\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"Epoch: \"+str(epoch+1))\n",
    "        \n",
    "        #training\n",
    "        model.train()\n",
    "        for batchID, (input, target) in enumerate (train_dataloader):\n",
    "            varInput = torch.autograd.Variable(input)\n",
    "            varTarget = torch.autograd.Variable(target)         \n",
    "            varOutput = model(varInput)\n",
    "            \n",
    "            lossvalue = loss(varOutput, varTarget)\n",
    "            print(lossvalue)\n",
    "                       \n",
    "            optimizer.zero_grad()\n",
    "            lossvalue.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        #validation\n",
    "        model.eval()\n",
    "        counter = 0\n",
    "        loss_sum = 0\n",
    "        for i, (input, target) in enumerate (val_dataloader):     \n",
    "            varInput = torch.autograd.Variable(input, volatile=True)\n",
    "            varTarget = torch.autograd.Variable(target, volatile=True)    \n",
    "            varOutput = model(varInput)\n",
    "            \n",
    "            losstensor = loss(varOutput, varTarget)\n",
    "            loss_sum += losstensor.item()\n",
    "            counter += 1\n",
    "            \n",
    "        loss_mean = loss_sum / counter\n",
    "        time_end = time.strftime('%m%d_%H%M%S')\n",
    "        \n",
    "        scheduler.step(loss_mean)\n",
    "        \n",
    "        if loss_mean < loss_mean_min:\n",
    "            loss_mean_min = loss_mean\n",
    "            torch.save({'epoch': epoch + 1,\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict()},\n",
    "                       './checkpoints/m_' + time_end + '.pth.tar')\n",
    "            print ('Epoch [' + str(epoch+1) + '] [save] [' + time_end + '] loss= ' + str(loss_mean))\n",
    "        else:\n",
    "            print ('Epoch [' + str(epoch+1) + '] [----] [' + time_end + '] loss= ' + str(loss_mean))\n",
    "        print('--------------------------------------------------------------------------\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "DN121 = densenet121()\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 3\n",
    "\n",
    "train_test_data = DatasetGenerator(csv_train_path)\n",
    "test_data, train_data = random_split(dataset, [500, len(dataset) - 500])\n",
    "val_data = DatasetGenerator(csv_valid_path)\n",
    "    \n",
    "train_dataloader = DataLoader(train_data, BATCH_SIZE, shuffle=True, num_workers=24, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_data, BATCH_SIZE, shuffle=False, num_workers=24, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_data, BATCH_SIZE, shuffle=False, num_workers=24, pin_memory=True)\n",
    "\n",
    "train(DN121, BATCH_SIZE, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUROC(gt, pred):\n",
    "    AUROCS = []\n",
    "\n",
    "    npgt = gt.numpy()\n",
    "    nppred = pred.numpy()\n",
    "\n",
    "    for i in range(len(CLASSES)):\n",
    "        try:\n",
    "            AUROCS.append(roc_auc_score(npgt[:, i], nppred[:, i]))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return AUROCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):   \n",
    "    gt = torch.FloatTensor()\n",
    "    pred = torch.FloatTensor()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(test_dataloader):\n",
    "            target = target.cuda()\n",
    "            gt = torch.cat((outGT, target), 0)\n",
    "\n",
    "            bs, c, h, w = input.size()\n",
    "            varInput = input.view(-1, c, h, w)\n",
    "\n",
    "            output = model(varInput)\n",
    "            pred = torch.cat((pred, output), 0)\n",
    "\n",
    "    AUROCS = compute_AUROC(gt, pred)\n",
    "    AUROC_mean = np.array(AUROCS).mean()\n",
    "\n",
    "    print ('AUROC mean: '+str(AUROC_mean))\n",
    "\n",
    "    for i in range (0, len(AUROCS)):\n",
    "        print (CLASSES[i], ' ', AUROCS[i])\n",
    "\n",
    "    return gt, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target, pred = test(DN121)\\n\\nfor i in range(len(CLASSES)):\\n    fpr, tpr, threshold = metrics.roc_curve(target[:,i], pred[:,i])\\n    roc_auc = metrics.auc(fpr, tpr)\\n    f = plt.subplot(2, 7, i+1)\\n\\n    plt.title(\\'ROC for: \\' + class_names[i])\\n    plt.plot(fpr, tpr, label = \\'U-ones: AUC = %0.2f\\' % roc_auc)\\n\\n    plt.legend(loc = \\'lower right\\')\\n    plt.plot([0, 1], [0, 1],\\'r--\\')\\n    plt.xlim([0, 1])\\n    plt.ylim([0, 1])\\n    plt.ylabel(\\'True Positive Rate\\')\\n    plt.xlabel(\\'False Positive Rate\\')\\n\\nfig_size = plt.rcParams[\"figure.figsize\"]\\nfig_size[0] = 30\\nfig_size[1] = 10\\nplt.rcParams[\"figure.figsize\"] = fig_size\\nplt.show()'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target, pred = test(DN121)\n",
    "\n",
    "for i in range(len(CLASSES)):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(target[:,i], pred[:,i])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    f = plt.subplot(2, 7, i+1)\n",
    "\n",
    "    plt.title('ROC for: ' + class_names[i])\n",
    "    plt.plot(fpr, tpr, label = 'U-ones: AUC = %0.2f' % roc_auc)\n",
    "\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 30\n",
    "fig_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
